{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,mean_squared_error,mean_absolute_error,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures,LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "rm_stopword = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Height and Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htnwt():\n",
    "    df = pd.read_csv(\"train_hw.csv\")\n",
    "    df.drop('Id',axis=1,inplace=True)\n",
    "    X = df[['height']]\n",
    "    y = df['weight']\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,test_size=0.3,random_state=21)\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    pf = PolynomialFeatures(degree=2)\n",
    "    X_new_train = pf.fit_transform(X_train)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_new_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigMart Sales Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigmrt():\n",
    "    df_train = pd.read_csv(\"train_bgmrt.csv\")\n",
    "    df_train.fillna(df_train.Item_Weight.mean(),inplace=True)\n",
    "    df_train.drop([\"Item_Identifier\",\"Outlet_Establishment_Year\",\"Outlet_Size\"],axis=1,inplace=True)\n",
    "    col = [\"Item_Fat_Content\",\"Item_Type\",\"Outlet_Identifier\",\"Outlet_Location_Type\",\"Outlet_Type\"]\n",
    "    global le\n",
    "    for c in col:\n",
    "        le = LabelEncoder()\n",
    "        df_train[c] = le.fit_transform(df_train[c])\n",
    "    X = df_train.iloc[:,:-1].values\n",
    "    y = df_train.iloc[:,-1].values\n",
    "    #X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,train_size=0.75,random_state=11)\n",
    "    global ss\n",
    "    ss = StandardScaler()\n",
    "    Xt_train = ss.fit_transform(X)\n",
    "    #Xt_test = ss.transform(X_test)\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    global abr\n",
    "    abr = AdaBoostRegressor(n_estimators=8,loss='linear',random_state=15)\n",
    "    abr.fit(Xt_train,y)\n",
    "\n",
    "def pred_bgmrt(X_test):\n",
    "    X_test = le.transform(X_test)\n",
    "    Xt_test = ss.transform(X_test)\n",
    "    global abr\n",
    "    pred = abr.predict(Xt_test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC News Category Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(doc):\n",
    "        doc = doc.lower()\n",
    "        doc = re.sub(f\"[{string.punctuation}]\",\"\",doc)\n",
    "        doc_splt = doc.split()\n",
    "        newdoc = []\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for token in doc_splt:\n",
    "            if token not in rm_stopword:\n",
    "                newdoc.append(wnl.lemmatize(token))\n",
    "        return \" \".join(newdoc)\n",
    "\n",
    "def bbcnews():\n",
    "    df_train = pd.read_csv(\"train_bbc.csv\")\n",
    "    X_train = df_train.Text.values\n",
    "    corpus_train = list(map(text_cleaning,X_train))\n",
    "    yt_train = df_train['Category'].values\n",
    "    \n",
    "    global tv\n",
    "    tv = TfidfVectorizer()\n",
    "    corpus_t_train = tv.fit_transform(corpus_train).toarray()\n",
    "    global model_mb\n",
    "    model_mb = MultinomialNB()\n",
    "    model_mb.fit(corpus_t_train,yt_train)\n",
    "    \n",
    "def pred_bbcnews():\n",
    "    global tv\n",
    "    sample_test_X = list(map(text_cleaning,sample_test_X))\n",
    "    sample_test_X = tv.transform(sample_test_X).toarray()\n",
    "    global model_mb\n",
    "    pred = model_mb.predict(sample_test_X)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotten Tomatos Movies sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt():\n",
    "    df = pd.read_csv(\"train_rt.tsv\",delimiter=\"\\t\")\n",
    "    df.drop(['PhraseId','SentenceId'],inplace=True,axis=1)\n",
    "    ret = RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n",
    "    global cv\n",
    "    cv = CountVectorizer(lowercase=True,stop_words='english',tokenizer=ret.tokenize)\n",
    "    X = cv.fit_transform(df['Phrase'])\n",
    "    y = df['Sentiment']\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,train_size=0.8,random_state=22)\n",
    "    global mb_rt\n",
    "    mb_rt = MultinomialNB()\n",
    "    mb_rt.fit(X_train,y_train)\n",
    "    print(\"Training Score: \",mb_rt.score(X_train,y_train))\n",
    "    print(\"Testing Score: \",mb_rt.score(X_test,y_test))\n",
    "\n",
    "def pred_rt(X_test):\n",
    "    global cv,mb_rt\n",
    "    X_test = cv.transform(X_test)\n",
    "    pred = mb_rt.predict(X_test)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan Approval Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loan_apprv():\n",
    "    df = pd.read_csv(\"loan_approval.csv\")\n",
    "    df.drop(['Loan_ID','Loan_Amount_Term'],axis=1,inplace=True)\n",
    "    df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])\n",
    "    df['Self_Employed'] = df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])\n",
    "    df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])\n",
    "    df['Married'] = df['Married'].fillna(df['Married'].mode()[0])\n",
    "    df['Dependents'] = df['Dependents'].fillna(df['Dependents'].mode()[0])\n",
    "    df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())\n",
    "    c = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area']\n",
    "    global le\n",
    "    le = LabelEncoder()\n",
    "    for col in c:\n",
    "        if df[col].dtypes == 'object':\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "    X = df.iloc[:,:-1].values\n",
    "    y = df.iloc[:,-1].values\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=10)\n",
    "    global ss\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    from sklearn.svm import SVC\n",
    "    global model_svc\n",
    "    model_svc = SVC(kernel='linear',probability=True)\n",
    "    model_svc.fit(X_train,y_train)\n",
    "\n",
    "    pred_test = model_svc.predict(X_test)\n",
    "    acs_test= accuracy_score(y_test,pred_test)\n",
    "    acs_train = accuracy_score(y_train,model_svc.predict(X_train))\n",
    "    print(\"Accuracy Score Train:\",acs_train)\n",
    "    print(\"Accuracy Score Test:\",acs_test)\n",
    "\n",
    "def pred_loan(X_test):\n",
    "    global model_svc,ss,le\n",
    "    X_test = le.transform(X_test)\n",
    "    Xt_test = ss.transform(X_test)\n",
    "    pred = model_svc.predict(Xt_test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb Movies Reviews Sentiment Analysis and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb():\n",
    "    df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "    X = df['review'].values\n",
    "    y = df['sentiment'].values\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.5,test_size=0.5,random_state=25)\n",
    "    reg = RegexpTokenizer(\"[a-zA-Z0-9+]\")\n",
    "    global cv\n",
    "    cv = CountVectorizer(lowercase=True,stop_words=\"english\",tokenizer=reg.tokenize)\n",
    "    Xt_train = cv.fit_transform(X_train).toarray()\n",
    "    Xt_test = cv.transform(X_test).toarray()\n",
    "    global mb_imdb\n",
    "    mb_imdb = MultinomialNB()\n",
    "    mb_imdb.fit(Xt_train,y_train)\n",
    "\n",
    "    pred = mb_imdb.predict(Xt_test)\n",
    "    acs_train = accuracy_score(y_train,mb_imdb.predict(Xt_train))\n",
    "    acs_test = accuracy_score(y_test,pred)\n",
    "\n",
    "    print(\"Accuracy Score Training: \",acs_train)\n",
    "    print(\"Accuracy Score Test: \",acs_test)\n",
    "\n",
    "def pred_imdb(X_test):\n",
    "    global cv,mb_imdb\n",
    "    X_test = cv.transform(X_test)\n",
    "    pred = mb_imdb.predict(X_test)\n",
    "    return pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
